#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Product Matching Project - Ana Dosya
ÃœrÃ¼n baÅŸlÄ±klarÄ±na dayalÄ± deep learning ile Ã¼rÃ¼n eÅŸleÅŸtirme projesi

Prd.md'deki yol haritasÄ±nÄ± takip ederek:
1. Veri analizi ve Ã¶n hazÄ±rlÄ±k
2. Text embedding (Sentence-BERT)
3. Deep learning ile benzerlik tespiti
4. Clustering algoritmalarÄ±
5. Post-processing
6. DeÄŸerlendirme metrikleri ve gÃ¶rselleÅŸtirme
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Deep Learning ve NLP
from sentence_transformers import SentenceTransformer
import torch
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import umap

# Clustering
from sklearn.cluster import DBSCAN, AgglomerativeClustering, KMeans
from sklearn.metrics.pairwise import cosine_similarity, cosine_distances

# DeÄŸerlendirme metrikleri
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    adjusted_rand_score, normalized_mutual_info_score, silhouette_score,
    confusion_matrix, classification_report
)

# Metin iÅŸleme
import re
import string
from collections import Counter

# DiÄŸer
import warnings
warnings.filterwarnings('ignore')
import os # gorseller klasÃ¶rÃ¼ oluÅŸturmak iÃ§in

# GÃ¶rselleÅŸtirme ayarlarÄ±
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 12

# Gerekli ek kÃ¼tÃ¼phaneler
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from sklearn.cluster import SpectralClustering
import hdbscan
nltk.download('stopwords')
nltk.download('wordnet')

import itertools

# Add TensorFlow import and availability flags
_torch_available = True # Assuming torch was successfully imported
_tf_available = False
try:
    import tensorflow as tf
    _tf_available = True
except ImportError:
    print("TensorFlow not found. Some functionalities might be limited.")
    pass # TensorFlow is optional if not using TF models directly without sentence-transformers handling it

# Attempt to import torch and tensorflow for robust tensor handling
_torch_imported = False
try:
    import torch
    _torch_imported = True
except ImportError:
    pass

_tensorflow_imported = False
try:
    import tensorflow as tf
    _tensorflow_imported = True
except ImportError:
    pass

class ProductMatcher:
    """
    ÃœrÃ¼n eÅŸleÅŸtirme iÃ§in ana sÄ±nÄ±f
    """
    
    def __init__(self, data_path='sample_data.csv'):
        """
        SÄ±nÄ±fÄ± baÅŸlat ve veriyi yÃ¼kle
        """
        self.data_path = data_path
        self.df = None
        self.embeddings = None
        self.model = None
        self.results = {}
        self.gorseller_path = "gorseller" # GÃ¶rseller klasÃ¶rÃ¼ yolu
        if not os.path.exists(self.gorseller_path):
            os.makedirs(self.gorseller_path)
            print(f"ğŸ–¼ï¸ '{self.gorseller_path}' klasÃ¶rÃ¼ oluÅŸturuldu.")
        
    def load_and_analyze_data(self):
        """
        1. AÅŸama: Veri yÃ¼kleme ve analiz
        """
        print("ğŸ“¦ Veri yÃ¼kleniyor ve analiz ediliyor...")
        
        # Veriyi yÃ¼kle
        self.df = pd.read_csv(self.data_path)
        
        # SÃ¼tun adlarÄ±ndaki boÅŸluklarÄ± temizle
        self.df.columns = self.df.columns.str.strip()
        
        # Temel bilgiler
        print(f"âœ… Toplam kayÄ±t sayÄ±sÄ±: {len(self.df):,}")
        print(f"âœ… SÃ¼tunlar: {list(self.df.columns)}")
        print(f"âœ… Toplam benzersiz kÃ¼me sayÄ±sÄ±: {self.df['Cluster ID'].nunique():,}")
        print(f"âœ… Toplam kategori sayÄ±sÄ±: {self.df['Category ID'].nunique():,}")
        
        # Eksik veri kontrolÃ¼
        print("\nğŸ” Eksik veri analizi:")
        missing_data = self.df.isnull().sum()
        print(missing_data[missing_data > 0])
        
        # BaÅŸlÄ±k uzunluklarÄ±nÄ±n analizi
        self.df['title_length'] = self.df['Product Title'].str.len()
        
        print(f"\nğŸ“Š BaÅŸlÄ±k uzunluk istatistikleri:")
        print(f"Ortalama: {self.df['title_length'].mean():.1f}")
        print(f"Minimum: {self.df['title_length'].min()}")
        print(f"Maksimum: {self.df['title_length'].max()}")
        
        return self.df
    
    def preprocess_titles(self):
        """
        2. AÅŸama: BaÅŸlÄ±k Ã¶n iÅŸleme (Ä°ngilizce iÃ§in sadeleÅŸtirilmiÅŸ)
        """
        print("\nğŸ§½ BaÅŸlÄ±klar temizleniyor (Ä°ngilizce)...")
        import unicodedata
        from nltk.corpus import stopwords
        from nltk.stem import WordNetLemmatizer
        import inflect

        stop_words = set(stopwords.words('english'))
        lemmatizer = WordNetLemmatizer()
        p = inflect.engine()

        # Marka/model listesi Ã¶rnek (geliÅŸtirilebilir)
        known_brands = ["samsung", "apple", "lg", "bosch", "beko", "arcelik", "vestel", "siemens", "philips", "lenovo", "hp", "asus", "casper", "huawei", "xiaomi"]
        def normalize_brand(text):
            for brand in known_brands:
                if brand in text:
                    return brand + " " + text.replace(brand, "").strip()
            return text

        def number_to_words(text):
            # SayÄ±larÄ± yazÄ±ya Ã§evir (Ã¶r: 2 -> two)
            def repl(m):
                try:
                    return p.number_to_words(m.group())
                except:
                    return m.group()
            return re.sub(r'\b\d+\b', repl, text)

        def clean_title(title):
            if pd.isna(title):
                return ""
            # Unicode normalizasyonu
            title = unicodedata.normalize('NFKD', str(title))
            title = title.lower()
            # Noktalama ve Ã¶zel karakter temizliÄŸi
            title = re.sub(r'[^\w\s]', ' ', title)
            title = re.sub(r'\s+', ' ', title).strip()
            # SayÄ±larÄ± yazÄ±ya Ã§evir
            title = number_to_words(title)
            # Marka/model normalizasyonu
            title = normalize_brand(title)
            # Stopword temizliÄŸi
            words = [w for w in title.split() if w not in stop_words and len(w) > 1]
            # Ä°ngilizce lemmatization
            words = [lemmatizer.lemmatize(w) for w in words]
            return ' '.join(words)

        self.df['clean_title'] = self.df['Product Title'].apply(clean_title)
        initial_count = len(self.df)
        self.df = self.df[self.df['clean_title'].str.len() > 0]
        final_count = len(self.df)
        print(f"âœ… {initial_count - final_count} boÅŸ baÅŸlÄ±k temizlendi")
        print(f"âœ… Kalan kayÄ±t sayÄ±sÄ±: {final_count:,}")
        
    def generate_embeddings(self, model_name='all-MiniLM-L6-v2'):
        """
        3. AÅŸama: Text embedding oluÅŸturma
        Uses a Hugging Face model (all-MiniLM-L6-v2) by default.
        """
        print(f"\nğŸ¤– Sentence Transformers ile embedding oluÅŸturuluyor ({model_name})...")
        self.model = SentenceTransformer(model_name)
        titles = self.df['clean_title'].tolist()
        
        # SentenceTransformer's encode can return torch.Tensor, tf.Tensor, or np.ndarray
        # based on availability and the model type.
        embeddings_output = self.model.encode(
            titles, 
            convert_to_tensor=True, # Tries to return native tensor type
            show_progress_bar=True,
            batch_size=32 # Adjusted batch size, can be tuned
        )
        
        # Robustly convert to NumPy array
        if _torch_imported and isinstance(embeddings_output, torch.Tensor):
            print("ğŸ”© Embeddings converted from PyTorch tensor to NumPy array.")
            self.embeddings = embeddings_output.cpu().numpy()
        elif _tensorflow_imported and tf.is_tensor(embeddings_output):
            print("ğŸ”© Embeddings converted from TensorFlow tensor to NumPy array.")
            self.embeddings = embeddings_output.numpy()
        elif isinstance(embeddings_output, np.ndarray):
            print("ğŸ”© Embeddings are already NumPy array.")
            self.embeddings = embeddings_output
        else:
            # Fallback if it's a list of lists or other format (less likely with convert_to_tensor=True)
            print("ğŸ”© Embeddings converted from list/other to NumPy array.")
            self.embeddings = np.array(embeddings_output)
            
        print(f"âœ… Embedding boyutu: {self.embeddings.shape}")
        return self.embeddings
    
    def cluster_products(self, algorithms=['dbscan', 'agglomerative', 'kmeans', 'spectral', 'hdbscan']):
        """
        4. AÅŸama: Clustering algoritmalarÄ±
        """
        print("\nğŸ§© Clustering algoritmalarÄ± uygulanÄ±yor...")
        clustering_results = {}
        true_n_clusters = self.df['Cluster ID'].nunique()
        for algorithm in algorithms:
            print(f"\nâ–¶ï¸ {algorithm.upper()} uygulanÄ±yor...")
            if algorithm == 'dbscan':
                clusterer = DBSCAN(eps=0.15, min_samples=2, metric='cosine')
                cluster_labels = clusterer.fit_predict(self.embeddings)
            elif algorithm == 'agglomerative':
                clusterer = AgglomerativeClustering(
                    n_clusters=true_n_clusters,
                    metric='cosine',
                    linkage='average'
                )
                cluster_labels = clusterer.fit_predict(self.embeddings)
            elif algorithm == 'kmeans':
                clusterer = KMeans(
                    n_clusters=true_n_clusters,
                    random_state=42,
                    n_init=10
                )
                cluster_labels = clusterer.fit_predict(self.embeddings)
            elif algorithm == 'spectral':
                clusterer = SpectralClustering(
                    n_clusters=true_n_clusters,
                    affinity='nearest_neighbors',
                    random_state=42
                )
                cluster_labels = clusterer.fit_predict(self.embeddings)
            elif algorithm == 'hdbscan':
                clusterer = hdbscan.HDBSCAN(min_cluster_size=5, metric='euclidean')
                cluster_labels = clusterer.fit_predict(self.embeddings)
            clustering_results[algorithm] = cluster_labels
            n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
            print(f"âœ… Bulunan kÃ¼me sayÄ±sÄ±: {n_clusters}")
        self.results['clustering'] = clustering_results
        return clustering_results
    
    def evaluate_performance(self):
        """
        5. AÅŸama: Model performansÄ±nÄ± deÄŸerlendirme
        """
        print("\nğŸ“Š Model performansÄ± deÄŸerlendiriliyor...")
        
        evaluation_results = {}
        true_labels = self.df['Cluster ID'].values
        
        for algorithm, predicted_labels in self.results['clustering'].items():
            print(f"\nğŸ” {algorithm.upper()} DeÄŸerlendirmesi:")
            
            # Clustering metrikleri
            ari = adjusted_rand_score(true_labels, predicted_labels)
            nmi = normalized_mutual_info_score(true_labels, predicted_labels)
            
            try:
                silhouette = silhouette_score(self.embeddings, predicted_labels)
            except:
                silhouette = 0.0
            
            # Binary classification olarak deÄŸerlendirme iÃ§in pairwise hesaplama
            # Bu adÄ±m memory-intensive olabilir, sample alÄ±yoruz
            sample_size = min(5000, len(self.df))
            sample_indices = np.random.choice(len(self.df), sample_size, replace=False)
            
            y_true_pairs = []
            y_pred_pairs = []
            
            for i in range(sample_size):
                for j in range(i+1, sample_size):
                    idx_i, idx_j = sample_indices[i], sample_indices[j]
                    
                    # True labels: aynÄ± cluster'da mÄ±?
                    true_same = (true_labels[idx_i] == true_labels[idx_j])
                    pred_same = (predicted_labels[idx_i] == predicted_labels[idx_j])
                    
                    y_true_pairs.append(int(true_same))
                    y_pred_pairs.append(int(pred_same))
            
            # Binary classification metrikleri
            accuracy = accuracy_score(y_true_pairs, y_pred_pairs)
            precision = precision_score(y_true_pairs, y_pred_pairs, average='binary')
            recall = recall_score(y_true_pairs, y_pred_pairs, average='binary')
            f1 = f1_score(y_true_pairs, y_pred_pairs, average='binary')
            
            # Sensitivity (Recall) ve Specificity hesaplama
            tn, fp, fn, tp = confusion_matrix(y_true_pairs, y_pred_pairs).ravel()
            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
            
            results = {
                'ARI': ari,
                'NMI': nmi,
                'Silhouette': silhouette,
                'Accuracy': accuracy,
                'Precision': precision,
                'Recall': recall,
                'F1-Score': f1,
                'Sensitivity': sensitivity,
                'Specificity': specificity
            }
            
            evaluation_results[algorithm] = results
            
            # SonuÃ§larÄ± yazdÄ±r
            print(f"  ğŸ“ˆ ARI: {ari:.4f}")
            print(f"  ğŸ“ˆ NMI: {nmi:.4f}")
            print(f"  ğŸ“ˆ Silhouette: {silhouette:.4f}")
            print(f"  ğŸ“ˆ Accuracy: {accuracy:.4f}")
            print(f"  ğŸ“ˆ Precision: {precision:.4f}")
            print(f"  ğŸ“ˆ Recall (Sensitivity): {recall:.4f}")
            print(f"  ğŸ“ˆ F1-Score: {f1:.4f}")
            print(f"  ğŸ“ˆ Specificity: {specificity:.4f}")
        
        self.results['evaluation'] = evaluation_results
        return evaluation_results
    
    def visualize_results(self):
        """
        6. AÅŸama: SonuÃ§larÄ± gÃ¶rselleÅŸtirme
        """
        print("\nğŸ¨ SonuÃ§lar gÃ¶rselleÅŸtiriliyor...")
        
        # 1. Embedding'leri 2D'ye indirgeme (UMAP)
        print("ğŸ“Š UMAP ile embedding gÃ¶rselleÅŸtirmesi...")
        reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)
        embeddings_2d = reducer.fit_transform(self.embeddings)
        
        # 2. t-SNE ile de gÃ¶rselleÅŸtirme
        print("ğŸ“Š t-SNE ile embedding gÃ¶rselleÅŸtirmesi...")
        tsne = TSNE(n_components=2, random_state=42, perplexity=30)
        # BÃ¼yÃ¼k veri setleri iÃ§in sample alÄ±yoruz
        sample_size = min(3000, len(self.embeddings))
        sample_indices = np.random.choice(len(self.embeddings), sample_size, replace=False)
        embeddings_tsne = tsne.fit_transform(self.embeddings[sample_indices])
        
        # GÃ¶rselleÅŸtirme fonksiyonlarÄ±nÄ± Ã§aÄŸÄ±r
        self._plot_embedding_visualization(embeddings_2d, 'UMAP')
        self._plot_tsne_visualization(embeddings_tsne, sample_indices, 't-SNE')
        self._plot_performance_comparison()
        self._plot_cluster_distribution()
        self._plot_confusion_matrices()
        
    def _plot_embedding_visualization(self, embeddings_2d, method_name):
        """UMAP/t-SNE gÃ¶rselleÅŸtirmesi"""
        fig, axes = plt.subplots(1, 2, figsize=(20, 8))
        
        # GerÃ§ek kÃ¼meler
        scatter1 = axes[0].scatter(
            embeddings_2d[:, 0], embeddings_2d[:, 1], 
            c=self.df['Cluster ID'], 
            cmap='tab20', 
            alpha=0.6, 
            s=50
        )
        axes[0].set_title(f'{method_name} - GerÃ§ek KÃ¼meler', fontsize=14, fontweight='bold')
        axes[0].set_xlabel(f'{method_name} 1')
        axes[0].set_ylabel(f'{method_name} 2')
        plt.colorbar(scatter1, ax=axes[0])
        
        # Tahmin edilen kÃ¼meler (DBSCAN)
        if 'dbscan' in self.results['clustering']:
            scatter2 = axes[1].scatter(
                embeddings_2d[:, 0], embeddings_2d[:, 1], 
                c=self.results['clustering']['dbscan'], 
                cmap='tab20', 
                alpha=0.6, 
                s=50
            )
            axes[1].set_title(f'{method_name} - DBSCAN Tahminleri', fontsize=14, fontweight='bold')
            axes[1].set_xlabel(f'{method_name} 1')
            axes[1].set_ylabel(f'{method_name} 2')
            plt.colorbar(scatter2, ax=axes[1])
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.gorseller_path, f'embedding_visualization_{method_name.lower()}.png'), dpi=300, bbox_inches='tight')
        plt.show()
    
    def _plot_tsne_visualization(self, embeddings_tsne, sample_indices, method_name):
        """t-SNE gÃ¶rselleÅŸtirmesi (sample verilerle)"""
        fig, axes = plt.subplots(1, 2, figsize=(20, 8))
        
        sample_true_labels = self.df['Cluster ID'].iloc[sample_indices]
        
        # GerÃ§ek kÃ¼meler
        scatter1 = axes[0].scatter(
            embeddings_tsne[:, 0], embeddings_tsne[:, 1], 
            c=sample_true_labels, 
            cmap='tab20', 
            alpha=0.6, 
            s=50
        )
        axes[0].set_title(f'{method_name} - GerÃ§ek KÃ¼meler (Sample)', fontsize=14, fontweight='bold')
        axes[0].set_xlabel(f'{method_name} 1')
        axes[0].set_ylabel(f'{method_name} 2')
        plt.colorbar(scatter1, ax=axes[0])
        
        # Tahmin edilen kÃ¼meler
        if 'dbscan' in self.results['clustering']:
            sample_pred_labels = np.array(self.results['clustering']['dbscan'])[sample_indices]
            scatter2 = axes[1].scatter(
                embeddings_tsne[:, 0], embeddings_tsne[:, 1], 
                c=sample_pred_labels, 
                cmap='tab20', 
                alpha=0.6, 
                s=50
            )
            axes[1].set_title(f'{method_name} - DBSCAN Tahminleri (Sample)', fontsize=14, fontweight='bold')
            axes[1].set_xlabel(f'{method_name} 1')
            axes[1].set_ylabel(f'{method_name} 2')
            plt.colorbar(scatter2, ax=axes[1])
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.gorseller_path, f'embedding_visualization_{method_name.lower()}.png'), dpi=300, bbox_inches='tight')
        plt.show()
    
    def _plot_performance_comparison(self):
        """Performans karÅŸÄ±laÅŸtÄ±rma grafikleri"""
        evaluation_results = self.results['evaluation']
        
        # Metrikleri dÃ¼zenle
        metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Sensitivity', 'Specificity', 'ARI', 'NMI']
        algorithms = list(evaluation_results.keys())
        
        # Performans karÅŸÄ±laÅŸtÄ±rma heatmap
        performance_matrix = []
        for algorithm in algorithms:
            row = [evaluation_results[algorithm][metric] for metric in metrics]
            performance_matrix.append(row)
        
        plt.figure(figsize=(12, 8))
        sns.heatmap(
            performance_matrix, 
            xticklabels=metrics, 
            yticklabels=[alg.upper() for alg in algorithms],
            annot=True, 
            fmt='.3f', 
            cmap='RdYlBu_r',
            center=0.5
        )
        plt.title('Algoritma Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontsize=16, fontweight='bold')
        plt.xlabel('Metrikler')
        plt.ylabel('Algoritmalar')
        plt.tight_layout()
        plt.savefig(os.path.join(self.gorseller_path, 'performance_heatmap.png'), dpi=300, bbox_inches='tight')
        plt.show()
        
        # Bar plot karÅŸÄ±laÅŸtÄ±rma
        fig, axes = plt.subplots(2, 4, figsize=(20, 10))
        axes = axes.flatten()
        
        for i, metric in enumerate(metrics):
            values = [evaluation_results[alg][metric] for alg in algorithms]
            axes[i].bar(algorithms, values, color=['skyblue', 'lightcoral', 'lightgreen'][:len(algorithms)])
            axes[i].set_title(f'{metric}', fontweight='bold')
            axes[i].set_ylabel('Skor')
            axes[i].set_ylim(0, 1)
            
            # DeÄŸerleri Ã§ubuklarÄ±n Ã¼stÃ¼ne yaz
            for j, v in enumerate(values):
                axes[i].text(j, v + 0.01, f'{v:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.gorseller_path, 'performance_comparison.png'), dpi=300, bbox_inches='tight')
        plt.show()
    
    def _plot_cluster_distribution(self):
        """KÃ¼me daÄŸÄ±lÄ±m grafikleri"""
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # GerÃ§ek kÃ¼me daÄŸÄ±lÄ±mÄ±
        cluster_counts = self.df['Cluster ID'].value_counts().head(20)
        axes[0, 0].bar(range(len(cluster_counts)), cluster_counts.values)
        axes[0, 0].set_title('GerÃ§ek KÃ¼me DaÄŸÄ±lÄ±mÄ± (Top 20)', fontweight='bold')
        axes[0, 0].set_xlabel('KÃ¼me ID')
        axes[0, 0].set_ylabel('ÃœrÃ¼n SayÄ±sÄ±')
        
        # Tahmin edilen kÃ¼me daÄŸÄ±lÄ±mlarÄ±
        for i, (algorithm, predicted_labels) in enumerate(self.results['clustering'].items()):
            if i >= 3:  # Maksimum 3 algoritma gÃ¶ster
                break
                
            unique, counts = np.unique(predicted_labels, return_counts=True)
            # Noise label'Ä± (-1) varsa ayÄ±r
            if -1 in unique:
                noise_idx = np.where(unique == -1)[0][0]
                noise_count = counts[noise_idx]
                unique = np.delete(unique, noise_idx)
                counts = np.delete(counts, noise_idx)
            else:
                noise_count = 0
            
            # En bÃ¼yÃ¼k 20 kÃ¼meyi gÃ¶ster
            sorted_indices = np.argsort(counts)[::-1][:20]
            top_counts = counts[sorted_indices]
            
            row, col = (0, 1) if i == 0 else ((1, 0) if i == 1 else (1, 1))
            axes[row, col].bar(range(len(top_counts)), top_counts)
            title = f'{algorithm.upper()} KÃ¼me DaÄŸÄ±lÄ±mÄ± (Top 20)'
            if noise_count > 0:
                title += f'\nNoise: {noise_count} Ã¼rÃ¼n'
            axes[row, col].set_title(title, fontweight='bold')
            axes[row, col].set_xlabel('KÃ¼me Index')
            axes[row, col].set_ylabel('ÃœrÃ¼n SayÄ±sÄ±')
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.gorseller_path, 'cluster_distributions.png'), dpi=300, bbox_inches='tight')
        plt.show()
    
    def _plot_confusion_matrices(self):
        """Confusion matrix gÃ¶rselleÅŸtirmeleri"""
        # Sample bir confusion matrix hesapla (pairwise comparison iÃ§in)
        sample_size = min(1000, len(self.df))
        sample_indices = np.random.choice(len(self.df), sample_size, replace=False)
        
        true_labels = self.df['Cluster ID'].iloc[sample_indices].values
        
        n_algorithms = len(self.results['clustering'])
        fig, axes = plt.subplots(1, n_algorithms, figsize=(6*n_algorithms, 5))
        
        if n_algorithms == 1:
            axes = [axes]
        
        for i, (algorithm, all_predicted_labels) in enumerate(self.results['clustering'].items()):
            predicted_labels = np.array(all_predicted_labels)[sample_indices]
            
            # Pairwise comparison iÃ§in binary labels oluÅŸtur
            y_true_pairs = []
            y_pred_pairs = []
            
            for j in range(min(500, sample_size)):  # Daha kÃ¼Ã§Ã¼k sample
                for k in range(j+1, min(500, sample_size)):
                    true_same = (true_labels[j] == true_labels[k])
                    pred_same = (predicted_labels[j] == predicted_labels[k])
                    
                    y_true_pairs.append(int(true_same))
                    y_pred_pairs.append(int(pred_same))
            
            cm = confusion_matrix(y_true_pairs, y_pred_pairs)
            
            sns.heatmap(
                cm, 
                annot=True, 
                fmt='d', 
                cmap='Blues',
                ax=axes[i],
                xticklabels=['FarklÄ± ÃœrÃ¼n', 'AynÄ± ÃœrÃ¼n'],
                yticklabels=['FarklÄ± ÃœrÃ¼n', 'AynÄ± ÃœrÃ¼n']
            )
            axes[i].set_title(f'{algorithm.upper()}\nConfusion Matrix', fontweight='bold')
            axes[i].set_xlabel('Tahmin Edilen')
            axes[i].set_ylabel('GerÃ§ek')
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.gorseller_path, 'confusion_matrices.png'), dpi=300, bbox_inches='tight')
        plt.show()
    
    def generate_report(self):
        """
        DetaylÄ± rapor oluÅŸturma
        """
        print("\nğŸ“‹ DetaylÄ± rapor oluÅŸturuluyor...")
        
        report = f"""
# ÃœRÃœN EÅLEÅTÄ°RME PROJESÄ° RAPORU

## ğŸ“Š VERÄ° SETÄ° BÄ°LGÄ°LERÄ°
- Toplam Ã¼rÃ¼n sayÄ±sÄ±: {len(self.df):,}
- Benzersiz kÃ¼me sayÄ±sÄ±: {self.df['Cluster ID'].nunique():,}
- Kategori sayÄ±sÄ±: {self.df['Category ID'].nunique():,}
- Ortalama baÅŸlÄ±k uzunluÄŸu: {self.df['title_length'].mean():.1f} karakter

## ğŸ¤– MODEL VE YÃ–NTEMLERÄ°
- Text Embedding: Sentence-BERT (all-MiniLM-L6-v2)
- Embedding boyutu: {self.embeddings.shape[1]}
- Clustering algoritmalarÄ±: {', '.join(self.results['clustering'].keys())}

## ğŸ“ˆ PERFORMANS SONUÃ‡LARI

"""
        
        for algorithm, metrics in self.results['evaluation'].items():
            report += f"### {algorithm.upper()}\n"
            for metric, value in metrics.items():
                report += f"- {metric}: {value:.4f}\n"
            report += "\n"
        
        report += """
## ğŸ¯ SONUÃ‡LAR VE Ã–NERÄ°LER

### En Ä°yi Performans GÃ¶steren Algoritma:
"""
        
        # En iyi algoritmanÄ± bul (F1-Score'a gÃ¶re)
        best_algorithm = max(
            self.results['evaluation'].keys(),
            key=lambda x: self.results['evaluation'][x]['F1-Score']
        )
        
        report += f"**{best_algorithm.upper()}** (F1-Score: {self.results['evaluation'][best_algorithm]['F1-Score']:.4f})\n\n"
        
        report += """
### Temel Bulgular:
1. Sentence-BERT embeddings Ã¼rÃ¼n baÅŸlÄ±klarÄ± iÃ§in etkili semantik temsil saÄŸlÄ±yor
2. Clustering performansÄ± ground truth ile karÅŸÄ±laÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda tatmin edici seviyede
3. FarklÄ± clustering algoritmalarÄ± farklÄ± gÃ¼Ã§lÃ¼ yÃ¶nler sergiliyor

### GeliÅŸtirme Ã–nerileri:
1. Hiperparametre optimizasyonu ile daha iyi sonuÃ§lar elde edilebilir
2. FarklÄ± embedding modelleri (e.g., multilingual models) denenebilir
3. Ensemble yÃ¶ntemleri ile performans artÄ±rÄ±labilir
4. Post-processing adÄ±mlarÄ± eklenebilir

---
Rapor Tarihi: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
        
        # Raporu dosyaya kaydet
        with open('product_matching_report.md', 'w', encoding='utf-8') as f:
            f.write(report)
        
        print("âœ… Rapor 'product_matching_report.md' dosyasÄ±na kaydedildi")
        return report
    
    def tune_clustering_algorithms(self):
        """
        DBSCAN, Agglomerative ve KMeans iÃ§in parametre arama ve karÅŸÄ±laÅŸtÄ±rmalÄ± rapor
        """
        print("\nğŸ”¬ KÃ¼meleme algoritmalarÄ± iÃ§in parametre aramasÄ± baÅŸlatÄ±lÄ±yor...")
        from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, f1_score
        true_labels = self.df['Cluster ID'].values
        results = []

        # 1. DBSCAN parametre grid search
        print("\nâ–¶ï¸ DBSCAN parametre aramasÄ±:")
        for eps in np.arange(0.05, 0.25, 0.02):
            for min_samples in [2, 3, 5, 7]:
                clusterer = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')
                cluster_labels = clusterer.fit_predict(self.embeddings)
                n_noise = (cluster_labels == -1).sum()
                n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
                ari = adjusted_rand_score(true_labels, cluster_labels)
                nmi = normalized_mutual_info_score(true_labels, cluster_labels)
                # F1-score (binary pairwise)
                try:
                    sample_size = min(500, len(self.df))
                    sample_indices = np.random.choice(len(self.df), sample_size, replace=False)
                    y_true_pairs, y_pred_pairs = [], []
                    for i in range(sample_size):
                        for j in range(i+1, sample_size):
                            idx_i, idx_j = sample_indices[i], sample_indices[j]
                            y_true_pairs.append(int(true_labels[idx_i] == true_labels[idx_j]))
                            y_pred_pairs.append(int(cluster_labels[idx_i] == cluster_labels[idx_j]))
                    f1 = f1_score(y_true_pairs, y_pred_pairs, average='binary')
                except:
                    f1 = 0.0
                print(f"  eps={eps}, min_samples={min_samples} | KÃ¼me: {n_clusters}, GÃ¼rÃ¼ltÃ¼: {n_noise}, ARI: {ari:.3f}, NMI: {nmi:.3f}, F1: {f1:.3f}")
                results.append({'alg':'DBSCAN', 'param':f'eps={eps},min_samples={min_samples}', 'clusters':n_clusters, 'noise':n_noise, 'ARI':ari, 'NMI':nmi, 'F1':f1})

        # 2. Agglomerative Clustering iÃ§in farklÄ± linkage
        print("\nâ–¶ï¸ Agglomerative Clustering parametre aramasÄ±:")
        for linkage in ['ward', 'complete', 'average', 'single']:
            # 'ward' sadece euclidean ile Ã§alÄ±ÅŸÄ±r
            if linkage == 'ward':
                clusterer = AgglomerativeClustering(n_clusters=20, linkage=linkage)
            else:
                clusterer = AgglomerativeClustering(n_clusters=20, linkage=linkage, metric='cosine')
            cluster_labels = clusterer.fit_predict(self.embeddings)
            n_clusters = len(set(cluster_labels))
            ari = adjusted_rand_score(true_labels, cluster_labels)
            nmi = normalized_mutual_info_score(true_labels, cluster_labels)
            try:
                sample_size = min(500, len(self.df))
                sample_indices = np.random.choice(len(self.df), sample_size, replace=False)
                y_true_pairs, y_pred_pairs = [], []
                for i in range(sample_size):
                    for j in range(i+1, sample_size):
                        idx_i, idx_j = sample_indices[i], sample_indices[j]
                        y_true_pairs.append(int(true_labels[idx_i] == true_labels[idx_j]))
                        y_pred_pairs.append(int(cluster_labels[idx_i] == cluster_labels[idx_j]))
                f1 = f1_score(y_true_pairs, y_pred_pairs, average='binary')
            except:
                f1 = 0.0
            print(f"  linkage={linkage} | KÃ¼me: {n_clusters}, ARI: {ari:.3f}, NMI: {nmi:.3f}, F1: {f1:.3f}")
            results.append({'alg':'Agglomerative', 'param':f'linkage={linkage}', 'clusters':n_clusters, 'noise':0, 'ARI':ari, 'NMI':nmi, 'F1':f1})

        # 3. KMeans iÃ§in farklÄ± kÃ¼me sayÄ±larÄ±
        print("\nâ–¶ï¸ KMeans parametre aramasÄ±:")
        for n_clusters in [10, 15, 20, 30, 50, 75, 100, 150, 200]:
            clusterer = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = clusterer.fit_predict(self.embeddings)
            ari = adjusted_rand_score(true_labels, cluster_labels)
            nmi = normalized_mutual_info_score(true_labels, cluster_labels)
            try:
                sample_size = min(500, len(self.df))
                sample_indices = np.random.choice(len(self.df), sample_size, replace=False)
                y_true_pairs, y_pred_pairs = [], []
                for i in range(sample_size):
                    for j in range(i+1, sample_size):
                        idx_i, idx_j = sample_indices[i], sample_indices[j]
                        y_true_pairs.append(int(true_labels[idx_i] == true_labels[idx_j]))
                        y_pred_pairs.append(int(cluster_labels[idx_i] == cluster_labels[idx_j]))
                f1 = f1_score(y_true_pairs, y_pred_pairs, average='binary')
            except:
                f1 = 0.0
            print(f"  n_clusters={n_clusters} | KÃ¼me: {n_clusters}, ARI: {ari:.3f}, NMI: {nmi:.3f}, F1: {f1:.3f}")
            results.append({'alg':'KMeans', 'param':f'n_clusters={n_clusters}', 'clusters':n_clusters, 'noise':0, 'ARI':ari, 'NMI':nmi, 'F1':f1})

        print("\nğŸ” Parametre arama tamamlandÄ±. SonuÃ§lar Ã¶zetlendi.")
        # SonuÃ§larÄ± DataFrame olarak kaydet
        pd.DataFrame(results).to_csv('clustering_param_search_results.csv', index=False)
        print("ğŸ’¾ SonuÃ§lar 'clustering_param_search_results.csv' dosyasÄ±na kaydedildi.")

    def postprocess_clusters(self, min_cluster_size=5):
        print("\nğŸ”§ KÃ¼Ã§Ã¼k kÃ¼meler birleÅŸtiriliyor...")
        for alg, labels in self.results['clustering'].items():
            unique, counts = np.unique(labels, return_counts=True)
            small_clusters = unique[counts < min_cluster_size]
            for sc in small_clusters:
                idx = np.where(labels == sc)[0]
                # En yakÄ±n bÃ¼yÃ¼k kÃ¼meye ata
                for i in idx:
                    # En yakÄ±n komÅŸunun kÃ¼mesini bul
                    dists = np.linalg.norm(self.embeddings[i] - self.embeddings, axis=1)
                    dists[idx] = np.inf
                    nearest = np.argmin(dists)
                    labels[i] = labels[nearest]
            self.results['clustering'][alg] = labels
        print("âœ… KÃ¼Ã§Ã¼k kÃ¼meler birleÅŸtirildi.")

    def upm_style_category_threshold_analysis(self, thresholds=np.arange(0.1, 1.0, 0.1), algorithms=['kmeans', 'agglomerative', 'dbscan']):
        """
        Her kategori iÃ§in ve tÃ¼m veri iÃ§in farklÄ± similarity threshold'larÄ±nda clustering ve F1-score analizi yapar.
        SonuÃ§larÄ± UPM makalesindeki gibi Ã§izgi grafiÄŸiyle kaydeder.
        """
        print("\nğŸ”¬ UPM benzeri kategori & threshold analizi baÅŸlatÄ±lÄ±yor...")
        categories = self.df['Category Label'].unique()
        results = {cat: {alg: [] for alg in algorithms} for cat in categories}
        results['ALL'] = {alg: [] for alg in algorithms}

        for cat in itertools.chain(categories, ['ALL']):
            if cat == 'ALL':
                df_cat = self.df
                emb_cat = self.embeddings
                true_labels = df_cat['Cluster ID'].values
            else:
                df_cat = self.df[self.df['Category Label'] == cat]
                if len(df_cat) < 10:
                    continue  # Ã§ok az Ã¶rnekli kategori atlanÄ±r
                emb_cat = self.embeddings[df_cat.index]
                true_labels = df_cat['Cluster ID'].values
            for alg in algorithms:
                f1s = []
                for thresh in thresholds:
                    # KÃ¼meleme algoritmasÄ±nÄ± threshold ile uygula
                    if alg == 'dbscan':
                        clusterer = DBSCAN(eps=thresh, min_samples=2, metric='cosine')
                        pred_labels = clusterer.fit_predict(emb_cat)
                    elif alg == 'agglomerative':
                        n_clusters = len(np.unique(true_labels))
                        clusterer = AgglomerativeClustering(n_clusters=n_clusters, metric='cosine', linkage='average')
                        pred_labels = clusterer.fit_predict(emb_cat)
                    elif alg == 'kmeans':
                        n_clusters = len(np.unique(true_labels))
                        clusterer = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                        pred_labels = clusterer.fit_predict(emb_cat)
                    else:
                        continue
                    # Pairwise F1-score hesapla
                    sample_size = min(500, len(df_cat))
                    sample_indices = np.random.choice(len(df_cat), sample_size, replace=False)
                    y_true_pairs, y_pred_pairs = [], []
                    for i in range(sample_size):
                        for j in range(i+1, sample_size):
                            idx_i, idx_j = sample_indices[i], sample_indices[j]
                            y_true_pairs.append(int(true_labels[idx_i] == true_labels[idx_j]))
                            y_pred_pairs.append(int(pred_labels[idx_i] == pred_labels[idx_j]))
                    if len(y_true_pairs) > 0:
                        f1 = f1_score(y_true_pairs, y_pred_pairs, average='binary')
                    else:
                        f1 = 0.0
                    f1s.append(f1)
                results[cat][alg] = f1s
        # GÃ¶rselleÅŸtirme
        for cat in itertools.chain(categories, ['ALL']):
            if cat not in results or all(len(results[cat][alg]) == 0 for alg in algorithms):
                continue
            plt.figure(figsize=(10, 6))
            for alg in algorithms:
                plt.plot(thresholds, results[cat][alg], marker='o', label=alg.upper())
            plt.title(f'F1-Score vs Similarity Threshold\nKategori: {cat}')
            plt.xlabel('Similarity Threshold')
            plt.ylabel('F1-Score')
            plt.ylim(0, 1)
            plt.legend()
            plt.grid(True, linestyle='--', alpha=0.5)
            fname = os.path.join(self.gorseller_path, f'upm_style_f1_vs_threshold_{cat.replace(" ", "_").lower()}.png')
            plt.tight_layout()
            plt.savefig(fname, dpi=200)
            plt.close()
            print(f'âœ… Grafik kaydedildi: {fname}')

def main():
    """
    Ana fonksiyon - tÃ¼m pipeline'Ä± Ã§alÄ±ÅŸtÄ±r
    """
    print("ğŸš€ ÃœRÃœN EÅLEÅTÄ°RME PROJESÄ° BAÅLIYOR...")
    print("=" * 60)
    
    # ProductMatcher nesnesini oluÅŸtur
    matcher = ProductMatcher('sample_data.csv')
    
    try:
        # 1. Veri yÃ¼kleme ve analiz
        matcher.load_and_analyze_data()
        
        # 2. Ã–n iÅŸleme
        matcher.preprocess_titles()
        
        # 3. Embedding oluÅŸturma
        matcher.generate_embeddings()
        
        # --- PARAMETRE ARAMA ---
        matcher.tune_clustering_algorithms()
        
        # 4. Clustering
        matcher.cluster_products()
        
        # 5. DeÄŸerlendirme
        matcher.evaluate_performance()
        
        # 6. GÃ¶rselleÅŸtirme
        matcher.visualize_results()
        
        # 7. Rapor oluÅŸturma
        matcher.generate_report()
        
        # Post-processing
        matcher.postprocess_clusters()
        
        # UPM benzeri kategori & threshold analizi
        matcher.upm_style_category_threshold_analysis()
        
        print("\n" + "=" * 60)
        print("âœ… PROJENÄ°N TAMAMLANDI!")
        print("ğŸ¨ GÃ¶rselleÅŸtirmeler ve rapor oluÅŸturuldu")
        print("ğŸ“ Ã‡Ä±ktÄ± dosyalarÄ±:")
        print(f"   - {os.path.join(matcher.gorseller_path, 'embedding_visualization_umap.png')}")
        print(f"   - {os.path.join(matcher.gorseller_path, 'embedding_visualization_t-sne.png')}")
        print(f"   - {os.path.join(matcher.gorseller_path, 'performance_heatmap.png')}")
        print(f"   - {os.path.join(matcher.gorseller_path, 'performance_comparison.png')}")
        print(f"   - {os.path.join(matcher.gorseller_path, 'cluster_distributions.png')}")
        print(f"   - {os.path.join(matcher.gorseller_path, 'confusion_matrices.png')}")
        print("   - product_matching_report.md")
        print("   - clustering_param_search_results.csv")
        print(f"   - {os.path.join(matcher.gorseller_path, 'upm_style_f1_vs_threshold_*.png')}")
        
    except Exception as e:
        print(f"âŒ Hata oluÅŸtu: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 